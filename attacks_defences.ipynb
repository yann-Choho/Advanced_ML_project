{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTEBOOK FOR DEFENSE TECHNQUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\choho\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Skipping, found downloaded files in \".\\chinese-mnist\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "%run Chinese_MNIST_preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import of libraries\n",
    "# file management libraries\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "#libraries for data preprocessing\n",
    "import numpy as np\n",
    "\n",
    "#libraries for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#libraries for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Sets the seed for random number generators to ensure reproducibility.\n",
    "    \n",
    "    Parameters:\n",
    "    seed (int): The seed value to be used.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)  # Set the seed for PyTorch\n",
    "    torch.cuda.manual_seed(seed)  # Set the seed for CUDA (GPU)\n",
    "    torch.cuda.manual_seed_all(seed)  # Set the seed for all GPUs if using multiple GPUs\n",
    "    np.random.seed(seed)  # Set the seed for NumPy (used for random number generation outside PyTorch)\n",
    "    random.seed(seed)  # Set the seed for Python's random module\n",
    "\n",
    "# Example of usage\n",
    "set_seed(42)  # You can use any integer as a seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepating data format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very import when we use pytorch to well format the data (tensor, dataloader,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_set_data(set_set):\n",
    "    \"\"\"\n",
    "    Prepare the image tensors and labels from a given dataset.\n",
    "\n",
    "    Parameters:\n",
    "    set_set (list): The dataset containing images and labels.\n",
    "\n",
    "    Returns:\n",
    "    set_img_tensors (torch.Tensor): The image tensors of the dataset.\n",
    "    set_labels (torch.Tensor): The labels of the dataset.\n",
    "    \"\"\"\n",
    "    set_images = []\n",
    "    set_labels = []\n",
    "\n",
    "    # Separate the labels and images from the dataset\n",
    "    for i in range(len(set_set)):\n",
    "        set_images.append(set_set[i][0])\n",
    "        set_labels.append(int(set_set[i][1][-2:]) - 1)  # Subtract 1 here (so 0 corresponds to 01, 1 to 02, etc.) because the loss function expects 0-starting labels\n",
    "\n",
    "    set_img_tensors = torch.tensor(set_images, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "    set_labels = torch.tensor(set_labels, dtype=torch.long)\n",
    "\n",
    "    return set_img_tensors, set_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\choho\\AppData\\Local\\Temp\\ipykernel_17344\\2317009855.py:20: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  set_img_tensors = torch.tensor(set_images, dtype=torch.float32).permute(0, 3, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load the training and testing datasets in tensors format (images and labels) \n",
    "test_img_tensors, test_labels = prepare_set_data(test_set)\n",
    "train_img_tensors, train_labels = prepare_set_data(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put into dataloader because we will use mini-batch gradient descent\n",
    "\n",
    "# TRAINING SET\n",
    "train_dataset = TensorDataset(train_img_tensors, train_labels)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "# TEST SET\n",
    "test_dataset = TensorDataset(test_img_tensors, test_labels)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. BASELINE CNN Model for Chiness-Mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a CNN using built-in function of Pytorch for computation efficiency (Use of GPU, TPU) instead of using numpy from scratch which will have cause us take a\n",
    "long time for computation (even with library like numba for accelerating numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Define the CNN model using a python Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes=15):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        # Define the convolutional layers\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # Calculate the input size of the first linear layer based on the output of the convolutional layers\n",
    "        self.fc1_input_size = 32 * 16 * 16  # 32 channels, 16x16 size after max-pooling\n",
    "\n",
    "        # Define the fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.fc1_input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.reshape(x.size(0), -1) \n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNModel(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=8192, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=15, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the model\n",
    "model = CNNModel(num_classes=15)\n",
    "\n",
    "# Print the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Training the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function \n",
    "def train_one_epoch(model, dataloader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        images, labels = batch\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    return {'output': outputs,  # Pour afficher les pr√©dictions sur le dernier lot\n",
    "        'train_loss': total_loss / len(dataloader),\n",
    "        'train_acc': total_correct / total_samples,\n",
    "    }\n",
    "\n",
    "# Validation function \n",
    "def validate_one_epoch(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            \n",
    "    return {'output': outputs,  # Pour afficher les pr√©dictions sur le dernier lot\n",
    "        'val_loss': total_loss / len(dataloader),\n",
    "        'val_acc': total_correct / total_samples,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "skip_or_not_training = False  # Set this to True if you want to skip training and load the model directly\n",
    "\n",
    "\n",
    "if skip_or_not_training:\n",
    "    # Define the loss function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training and evaluation of the model on several epochs \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    num_epochs = 50\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # Define the optimizer\n",
    "\n",
    "\n",
    "    # Training and evaluation of the model on several epochs \n",
    "    for epoch in range(num_epochs):\n",
    "        train_result = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "        val_result = validate_one_epoch(model, test_loader, loss_fn, device)\n",
    "\n",
    "        # Display of results for each epoch \n",
    "        print(f'√âpoque {epoch+1}/{num_epochs}')\n",
    "        print(f'Entra√Ænement - Perte: {train_result[\"train_loss\"]:.4f}, Accuracy: {train_result[\"train_acc\"]:.4f}')\n",
    "        print(f'Validation - Perte: {val_result[\"val_loss\"]:.4f}, Accuracy: {val_result[\"val_acc\"]:.4f}')\n",
    "\n",
    "\n",
    "    # saving the model \n",
    "    torch.save(model.state_dict(), 'baseline_model.pth')\n",
    "\n",
    "    # load the model \n",
    "    model = CNNModel(num_classes=15)\n",
    "    model.load_state_dict(torch.load('baseline_model.pth'))\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNModel(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=8192, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=15, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CNNModel(num_classes=15)\n",
    "model.load_state_dict(torch.load('results/baseline_model.pth'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_predictions(model, dataloader, device):\n",
    "    \"\"\" function to get all predictions of a model on a dataset \n",
    "    parameters: \n",
    "    model: the model to use for prediction\n",
    "    dataloader: the dataset to predict\n",
    "    device: the device to use for prediction\n",
    "    returns:\n",
    "    all_predictions: the predictions of the model on the dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return all_predictions, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pr√©dictions: [11, 14, 9, 3, 9, 2, 4, 6, 9, 4, 8, 1, 11, 11, 7, 12, 13, 5, 1, 6, 14, 3, 11, 2, 14, 11, 10, 14, 1, 2, 7, 14, 9, 12, 3, 3, 5, 10, 13, 6, 7, 0, 6, 6, 2, 5, 12, 13, 1, 3, 6, 2, 9, 0, 14, 12, 0, 7, 8, 10, 13, 12, 8, 14, 3, 5, 7, 3, 2, 11, 12, 14, 1, 10, 4, 1, 3, 1, 11, 12, 2, 2, 7, 8, 4, 14, 0, 5, 4, 4, 12, 10, 0, 7, 14, 2, 0, 10, 14, 8, 5, 8, 5, 2, 3, 1, 1, 2, 9, 0, 13, 10, 1, 3, 9, 3, 4, 7, 13, 12, 11, 0, 5, 11, 2, 11, 2, 4, 7, 12, 11, 7, 9, 12, 3, 3, 5, 5, 8, 5, 3, 1, 0, 0, 9, 9, 14, 11, 9, 14, 4, 14, 1, 8, 2, 5, 3, 11, 13, 7, 5, 2, 10, 12, 11, 13, 13, 11, 12, 4, 11, 3, 14, 13, 8, 0, 8, 0, 3, 10, 4, 3, 7, 9, 1, 12, 0, 7, 8, 8, 14, 12, 11, 3, 3, 1, 5, 6, 7, 12, 6, 7, 6, 12, 6, 1, 12, 1, 11, 10, 11, 9, 9, 0, 11, 8, 5, 1, 6, 1, 6, 6, 13, 5, 4, 1, 4, 11, 2, 11, 5, 0, 9, 0, 6, 1, 12, 4, 11, 14, 3, 6, 8, 3, 5, 7, 0, 11, 13, 6, 14, 7, 1, 14, 8, 11, 14, 1, 11, 13, 2, 7, 5, 12, 9, 10, 6, 8, 8, 1, 2, 9, 14, 10, 4, 2, 11, 9, 14, 3, 10, 10, 13, 9, 7, 0, 3, 5, 13, 14, 7, 0, 5, 14, 14, 3, 5, 2, 3, 13, 7, 13, 10, 4, 12, 7, 9, 2, 5, 2, 12, 8, 11, 1, 0, 6, 10, 9, 0, 9, 10, 0, 5, 3, 9, 14, 1, 6, 13, 14, 9, 5, 12, 12, 2, 5, 1, 13, 10, 2, 3, 4, 0, 14, 6, 3, 5, 13, 2, 12, 1, 0, 1, 5, 6, 3, 1, 12, 13, 4, 2, 13, 11, 10, 6, 4, 13, 4, 11, 1, 8, 9, 13, 2, 12, 9, 9, 11, 11, 13, 13, 10, 5, 6, 8, 4, 11, 0, 6, 1, 7, 2, 6, 2, 1, 11, 5, 2, 4, 0, 1, 8, 3, 7, 10, 1, 8, 10, 8, 3, 14, 13, 12, 8, 0, 1, 5, 9, 3, 2, 6, 6, 6, 6, 7, 5, 9, 0, 9, 0, 13, 10, 7, 1, 8, 7, 9, 2, 13, 13, 1, 0, 2, 9, 10, 14, 3, 12, 9, 7, 3, 5, 7, 5, 1, 2, 0, 8, 10, 2, 5, 3, 5, 6, 12, 13, 13, 3, 12, 9, 9, 11, 5, 10, 9, 6, 10, 7, 6, 2, 2, 5, 13, 1, 5, 1, 4, 2, 2, 6, 12, 9, 0, 6, 1, 1, 11, 14, 5, 13, 14, 6, 7, 8, 5, 5, 8, 2, 3, 13, 14, 12, 1, 7, 5, 3, 8, 14, 9, 3, 1, 1, 8, 7, 13, 13, 11, 6, 1, 1, 3, 12, 11, 3, 2, 4, 10, 4, 7, 10, 2, 7, 10, 0, 8, 6, 10, 0, 9, 1, 9, 10, 1, 7, 0, 6, 7, 12, 3, 7, 12, 4, 8, 10, 1, 12, 11, 10, 1, 12, 5, 10, 7, 14, 10, 2, 10, 10, 4, 6, 5, 12, 11, 12, 5, 10, 13, 12, 11, 11, 4, 0, 10, 2, 0, 0, 7, 11, 7, 7, 8, 9, 7, 13, 12, 11, 0, 2, 0, 9, 3, 6, 5, 0, 0, 6, 14, 0, 11, 11, 0, 10, 7, 11, 1, 10, 14, 9, 6, 4, 5, 1, 12, 4, 12, 5, 14, 10, 1, 3, 2, 14, 12, 6, 9, 12, 9, 3, 11, 14, 2, 9, 2, 14, 10, 9, 4, 3, 6, 0, 3, 6, 13, 1, 2, 6, 5, 9, 5, 1, 1, 4, 0, 13, 5, 10, 2, 6, 8, 12, 8, 3, 14, 14, 13, 3, 8, 6, 14, 4, 4, 1, 9, 3, 4, 3, 12, 7, 10, 6, 10, 1, 0, 5, 12, 12, 9, 3, 14, 2, 12, 4, 0, 1, 1, 0, 10, 6, 14, 9, 14, 2, 13, 9, 5, 11, 9, 7, 8, 2, 1, 2, 0, 0, 1, 4, 8, 6, 3, 10, 5, 10, 11, 1, 0, 3, 4, 5, 0, 0, 14, 10, 3, 14, 5, 1, 5, 9, 14, 7, 0, 7, 4, 9, 7, 0, 4, 6, 5, 14, 0, 8, 5, 9, 12, 10, 11, 1, 3, 11, 6, 11, 4, 12, 7, 5, 6, 5, 0, 3, 7, 3, 1, 7, 7, 2, 3, 6, 9, 10, 14, 2, 7, 10, 7, 2, 12, 12, 5, 11, 4, 11, 3, 11, 4, 11, 14, 4, 0, 8, 13, 0, 13, 13, 1, 10, 10, 13, 8, 6, 12, 4, 7, 14, 9, 4, 1, 3, 3, 11, 5, 2, 12, 3, 3, 5, 8, 0, 7, 8, 7, 4, 5, 3, 1, 1, 2, 4, 14, 4, 6, 6, 4, 13, 2, 0, 0, 1, 10, 8, 13, 9, 3, 8, 4, 6, 14, 2, 4, 4, 8, 8, 3, 1, 3, 9, 10, 2, 2, 1, 4, 3, 4, 4, 0, 2, 9, 6, 12, 14, 14, 0, 11, 4, 3, 9, 5, 6, 12, 10, 8, 3, 2, 4, 6, 1, 6, 4, 2, 10, 7, 0, 10, 2, 9, 8, 3, 12, 13, 9, 8, 4, 2, 8, 0, 0, 12, 4, 0, 6, 13, 4, 1, 13, 13, 11, 6, 4, 4, 3, 9, 0, 14, 1, 3, 13, 11, 0, 14, 13, 3, 0, 12, 12, 7, 7, 3, 9, 12, 8, 9, 10, 5, 9, 1, 14, 13, 0, 4, 2, 2, 13, 13, 5, 4, 11, 4, 11, 11, 11, 2, 1, 1, 14, 10, 12, 10, 8, 11, 1, 12, 6, 6, 14, 10, 5, 12, 2, 4, 6, 8, 13, 5, 13, 5, 1, 7, 14, 0, 12, 11, 11, 11, 5, 9, 9, 8, 14, 11, 11, 8, 10, 0, 13, 4, 14, 8, 14, 5, 8, 13, 4, 0, 10, 7, 6, 11, 9, 14, 2, 1, 0, 4, 6, 8, 4, 10, 4, 3, 7, 2, 4, 14, 1, 2, 2, 4, 11, 14, 5, 9, 12, 2, 12, 11, 11, 7, 11, 5, 2, 12, 12, 1, 5, 11, 0, 9, 7, 5, 5, 5, 4, 7, 7, 2, 8, 2, 6, 10, 6, 7, 10, 7, 2, 9, 14, 10, 10, 12, 2, 1, 14, 11, 13, 7, 3, 9, 7, 2, 1, 3, 14, 2, 5, 7, 12, 5, 7, 9, 10, 3, 1, 12, 9, 8, 13, 12, 10, 6, 9, 7, 9, 7, 3, 11, 6, 6, 12, 1, 1, 5, 5, 4, 1, 0, 4, 7, 13, 1, 6, 3, 0, 5, 13, 13, 11, 3, 1, 3, 12, 11, 14, 3, 0, 14, 0, 11, 4, 11, 13, 1, 12, 8, 13, 12, 7, 11, 7, 0, 1, 13, 9, 11, 2, 5, 13, 9, 1, 6, 7, 11, 1, 1, 5, 0, 13, 8, 13, 8, 12, 11, 14, 11, 1, 12, 13, 1, 1, 8, 14, 14, 14, 10, 9, 11, 9, 14, 2, 6, 7, 9, 9, 10, 9, 2, 4, 14, 13, 10, 14, 10, 13, 4, 5, 3, 10, 14, 13, 4, 3, 8, 9, 9, 2, 6, 3, 4, 4, 6, 2, 0, 9, 9, 10, 2, 4, 3, 4, 11, 12, 8, 14, 8, 0, 6, 1, 8, 1, 12, 11, 3, 4, 11, 13, 14, 6, 13, 3, 3, 13, 8, 12, 5, 12, 6, 12, 8, 12, 9, 7, 0, 10, 11, 11, 12, 7, 3, 4, 1, 7, 10, 4, 3, 0, 12, 10, 5, 7, 14, 3, 14, 11, 3, 8, 13, 2, 5, 5, 1, 1, 13, 8, 7, 0, 9, 6, 6, 9, 1, 4, 5, 0, 2, 7, 9, 11, 2, 9, 0, 10, 12, 12, 6, 10, 11, 10, 9, 1, 0, 5, 1, 8, 5, 14, 6, 9, 9, 6, 13, 14, 1, 2, 7, 9, 9, 5, 0, 9, 5, 14, 10, 14, 2, 7, 3, 5, 1, 5, 4, 2, 12, 9, 0, 4, 9, 5, 9, 8, 10, 11, 4, 7, 0, 6, 3, 7, 5, 6, 7, 0, 1, 0, 9, 2, 5, 13, 5, 8, 4, 7, 4, 4, 7, 6, 13, 10, 9, 6, 0, 3, 6, 14, 5, 13, 6, 9, 7, 14, 6, 13, 13, 6, 2, 10, 6, 2, 10, 14, 4, 9, 3, 0, 12, 12, 3, 9, 1, 6, 5, 0, 10, 6, 3, 1, 12, 2, 8, 10, 5, 0, 10, 5, 7, 8, 2, 1, 8, 4, 2, 1, 13, 3, 12, 6, 13, 13, 7, 6, 11, 13, 5, 9, 9, 6, 14, 6, 8, 2, 1, 8, 10, 11, 7, 12, 5, 5, 1, 13, 9, 8, 0, 9, 5, 1, 4, 2, 8, 0, 14, 6, 7, 7, 9, 12, 6, 7, 10, 8, 10, 2, 10, 2, 11, 3, 4, 0, 0, 13, 4, 4, 0, 8, 0, 7, 4, 3, 9, 2, 11, 11, 11, 1, 4, 10, 13, 3, 9, 6, 8, 0, 0, 7, 3, 1, 5, 8, 11, 6, 6, 14, 2, 13, 9, 9, 10, 7, 0, 0, 11, 6, 12, 4, 10, 12, 1, 10, 12, 7, 9, 13, 9, 2, 4, 9, 3, 11, 3, 7, 9, 1, 0, 4, 1, 5, 12, 11, 6, 1, 0, 2, 11, 8, 1, 14, 5, 0, 8, 1, 0, 4, 10, 8, 6, 13, 2, 11, 0, 14, 14, 6, 12, 3, 8, 7, 3, 7, 3, 1, 13, 0, 5, 10, 12, 3, 10, 2, 4, 12, 4, 2, 9, 8, 1, 13, 13, 11, 9, 9, 11, 4, 12, 9, 6, 1, 14, 4, 8, 12, 11, 4, 9, 8, 1, 1, 9, 10, 6, 14, 9, 14, 6, 7, 1, 4, 11, 5, 9, 9, 11, 9, 4, 10, 4, 7, 13, 13, 6, 6, 14, 6, 13, 4, 1, 2, 7, 4, 3, 10, 8, 0, 13, 3, 0, 2, 6, 10, 13, 6, 6, 14, 12, 11, 13, 8, 0, 2, 13, 7, 14, 3, 11, 13, 3, 3, 5, 12, 11, 14, 2, 10, 14, 8, 9, 13, 7, 13, 8, 12, 13, 10, 8, 7, 12, 12, 0, 10, 0, 6, 13, 10, 12, 4, 1, 8, 14, 10, 2, 12, 8, 2, 11, 5, 10, 4, 5, 3, 10, 12, 13, 7, 3, 8, 0, 14, 0, 12, 3, 12, 4, 6, 10, 4, 8, 11, 7, 3, 6, 3, 3, 14, 6, 7, 7, 11, 13, 10, 4, 12, 2, 5, 10, 7, 8, 10, 5, 5, 5, 4, 8, 13, 14, 13, 1, 10, 1, 13, 14, 11, 1, 10, 13, 7, 12, 3, 13, 7, 8, 7, 6, 4, 6, 12, 11, 12, 2, 8, 1, 10, 8, 10, 7, 1, 7, 13, 4, 8, 4, 3, 12, 14, 7, 10, 12, 7, 9, 6, 2, 10, 14, 9, 11, 4, 12, 7, 7, 12, 13, 12, 0, 12, 2, 6, 12, 11, 11, 3, 11, 5, 11, 4, 1, 12, 1, 11, 9, 2, 9, 13, 4, 4, 4, 6, 3, 8, 1, 4, 12, 14, 9, 13, 9, 10, 1, 3, 14, 3, 7, 8, 14, 14, 3, 12, 14, 3, 0, 0, 4, 11, 0, 1, 9, 7, 2, 5, 5, 13, 3, 8, 5, 5, 9, 14, 7, 6, 4, 7, 3, 2, 8, 7, 12, 0, 13, 6, 5, 11, 4, 14, 12, 1, 2, 14, 5, 11, 9, 3, 6, 8, 12, 8, 9, 4, 14, 12, 10, 11, 10, 13, 6, 0, 10, 6, 3, 11, 11, 7, 7, 0, 10, 2, 5, 13, 4, 5, 12, 2, 1, 4, 9, 9, 9, 10, 3, 4, 10, 2, 10, 5, 12, 6, 3, 13, 12, 6, 14, 14, 8, 6, 1, 14, 9, 0, 9, 13, 9, 9, 6, 5, 4, 0, 3, 5, 3, 11, 3, 13, 6, 8, 13, 3, 12, 10, 1, 8, 7, 3, 11, 7, 10, 9, 0, 8, 7, 13, 2, 10, 9, 9, 14, 2, 4, 5, 14, 7, 12, 7, 7, 0, 3, 11, 5, 3, 13, 12, 2, 13, 6, 11, 10, 14, 11, 12, 5, 2, 4, 4, 5, 14, 1, 6, 13, 14, 4, 11, 11, 9, 0, 2, 7, 0, 1, 8, 2, 2, 13, 10, 13, 2, 8, 5, 0, 14, 13, 0, 12, 0, 0, 3, 7, 5, 0, 6, 0, 1, 12, 10, 2, 1, 10, 0, 13, 10, 11, 0, 9, 14, 7, 12, 7, 10, 14, 2, 5, 8, 1, 1, 2, 8, 2, 14, 11, 11, 8, 11, 9, 8, 13, 14, 13, 2, 1, 14, 0, 2, 13, 1, 11, 3, 2, 4, 0, 1, 4, 3, 8, 3, 1, 1, 13, 2, 8, 6, 11, 2, 6, 10, 6, 2, 8, 0, 7, 5, 14, 10, 12, 4, 5, 6, 5, 0, 10, 9, 7, 12, 6, 4, 0, 12, 10, 4, 3, 5, 14, 13, 7, 14, 3, 13, 12, 10, 9, 5, 14, 12, 6, 12, 6, 5, 11, 6, 1, 5, 1, 14, 4, 10, 8, 12, 2, 14, 5, 14, 4, 6, 5, 7, 12, 9, 11, 13, 0, 1, 2, 6, 14, 9, 6, 1, 14, 8, 13, 8, 3, 14, 12, 8, 0, 8, 12, 0, 12, 4, 0, 0, 1, 4, 14, 11, 10, 5, 7, 1, 5, 8, 2, 13, 8, 11, 13, 8, 13, 5, 9, 3, 4, 10, 13, 0, 8, 3, 13, 13, 8, 12, 7, 11, 2, 13, 7, 0, 14, 0, 13, 3, 4, 9, 7, 6, 0, 11, 6, 6, 7, 0, 0, 7, 5, 8, 2, 11, 12, 3, 12, 4, 13, 13, 13, 12, 5, 14, 3, 3, 11, 7, 5, 13, 9, 6, 11, 9, 4, 10, 5, 13, 13, 1, 10, 8, 13, 0, 1, 7, 14, 14, 14, 12, 1, 1, 10, 13, 2, 14, 5, 4, 7, 6, 2, 7, 8, 11, 13, 1, 9, 4, 14, 7, 1, 13, 14, 7, 6, 6, 7, 7, 8, 14, 2, 3, 14, 8, 5, 14, 1, 2, 6, 5, 13, 13, 13, 14, 8, 13, 2, 11, 1, 11, 7, 8, 3, 8, 9, 0, 7, 13, 5, 11, 1, 14, 1, 13, 5, 6, 6, 8, 14, 9, 3, 14, 10, 10, 11, 2, 4, 1, 0, 5, 7, 11, 6, 7, 0, 6, 14, 7, 3, 10, 9, 14, 12, 8, 0, 13, 3, 10, 8, 3, 8, 6, 11, 10, 6, 10, 4, 12, 7, 4, 10, 11, 7, 0, 10, 2, 13, 0, 4, 4, 6, 6, 0, 2, 12, 7, 14, 5, 8, 13, 8, 14, 3, 7, 13, 0, 0, 7, 8, 10, 11, 14, 2, 2, 8, 6, 11, 7, 12, 6, 2, 13, 4, 8, 7, 1, 3, 6, 9, 10, 8, 8, 8, 2, 3, 11, 5, 12, 0, 10, 6, 4, 2, 8, 14, 8, 6, 11, 8, 11, 7, 6, 7, 14, 9, 0, 1, 4, 7, 2, 3, 9, 10, 9, 10, 1, 12, 4, 3, 3, 9, 8, 8, 4, 14, 0, 13, 9, 13, 7, 14, 13, 10, 5, 9, 9, 13, 8, 2, 10, 10, 5, 6, 10, 0, 10, 9, 3, 14, 14, 10, 9, 10, 3, 1, 8, 9, 14, 12, 14, 3, 1, 11, 11, 5, 11, 2, 6, 5, 0, 4, 5, 2, 4, 10, 6, 12, 10, 3, 3, 13, 1, 12, 2, 2, 5, 14, 0, 3, 2, 9, 4, 7, 9, 8, 7, 10, 7, 10, 5, 0, 11, 8, 10, 3, 12, 2, 11, 2, 9, 4, 6, 7, 5, 5, 6, 5, 0, 13, 1, 7, 0, 0, 10, 5, 5, 12, 3, 13, 1, 12, 7, 8, 6, 11, 3, 1, 6, 4, 0, 12, 12, 1, 5, 7, 0, 0, 9, 0, 12, 14, 8, 14, 8, 5, 3, 8, 3, 7, 14, 3, 6, 10, 1, 12, 8, 14, 5, 3, 8, 14, 3, 4, 9, 4, 0, 7, 9, 0, 8, 13, 5, 7, 6, 8, 0, 3, 6, 3, 5, 6, 0, 4, 6, 5, 2, 13, 0, 8, 14, 13, 8, 14, 1, 5, 1, 1, 8, 10, 7, 2, 5, 1, 2, 13, 7, 3, 12, 2, 8, 4, 13, 6, 2, 8, 5, 4, 1, 8, 3, 3, 4, 9, 6, 10, 2, 14, 5, 12, 3, 10, 1, 2, 11, 13, 10, 12, 5, 8, 13, 0, 7, 10, 5, 4, 6, 1, 9, 4, 14, 12, 0, 11, 1, 10, 8, 3, 6, 6, 9, 11, 4, 12, 6, 11, 4, 11, 11, 2, 11, 7, 12, 8, 5, 1, 8, 14, 1, 6, 5, 14, 2, 4, 3, 6, 0, 0, 2, 10, 10, 4, 7, 8, 3, 13, 5, 11, 12, 4, 2, 13, 10, 8, 7, 8, 2, 3, 8, 12, 1, 1, 6, 8, 5, 8, 14, 10, 11, 8, 10, 4, 13, 13, 0, 10, 10, 8, 10, 12, 2, 12, 9, 4, 2, 4, 3, 9, 6, 4, 9, 14, 1, 13, 1, 8, 8, 14, 3, 13, 11, 9, 14, 4, 7, 8, 10, 6, 14, 14, 0, 2, 11, 1, 14, 2, 5, 10, 12, 11, 11, 4, 8, 10, 1, 1, 2, 12, 9, 0, 5, 3, 11, 4, 9, 12, 4, 0, 5, 6, 6, 5, 13, 8, 9, 3, 4, 5, 0, 11, 2, 6, 7, 14, 7, 11, 11, 3, 14, 8, 5, 9, 8, 13, 11, 8, 6, 11, 3, 5, 9, 12, 0, 2, 0, 9, 12, 7, 3, 10, 9, 8, 13, 5, 4, 5, 14, 11, 7, 14, 14, 8, 11, 7, 7]\n",
      "√âtiquettes r√©elles: [11, 14, 9, 3, 9, 2, 4, 6, 9, 4, 8, 1, 11, 11, 7, 12, 13, 5, 1, 6, 14, 3, 5, 2, 14, 11, 10, 14, 1, 12, 7, 14, 9, 12, 3, 2, 5, 10, 13, 6, 7, 0, 6, 6, 2, 5, 12, 13, 1, 3, 6, 2, 9, 0, 14, 12, 0, 7, 8, 10, 13, 12, 8, 14, 3, 5, 7, 3, 2, 11, 12, 14, 1, 10, 4, 1, 3, 1, 11, 12, 2, 2, 7, 8, 4, 14, 0, 5, 4, 4, 12, 10, 0, 7, 14, 2, 0, 10, 4, 8, 5, 8, 5, 2, 3, 1, 1, 2, 9, 0, 13, 10, 1, 3, 9, 3, 4, 7, 13, 12, 11, 0, 5, 11, 2, 11, 2, 4, 7, 12, 11, 7, 9, 12, 3, 3, 5, 5, 8, 5, 3, 1, 0, 0, 9, 9, 14, 11, 9, 14, 4, 14, 1, 8, 2, 5, 3, 11, 13, 7, 5, 2, 10, 12, 11, 13, 13, 11, 12, 4, 11, 3, 14, 13, 8, 0, 8, 0, 3, 10, 4, 3, 7, 9, 1, 12, 0, 7, 8, 8, 14, 10, 11, 3, 3, 1, 5, 6, 7, 10, 6, 7, 6, 12, 6, 1, 12, 1, 13, 10, 11, 9, 9, 0, 11, 8, 5, 1, 6, 1, 6, 6, 13, 5, 9, 1, 4, 11, 2, 4, 5, 0, 9, 0, 6, 1, 12, 4, 11, 9, 3, 6, 8, 3, 11, 7, 0, 4, 13, 6, 14, 2, 1, 14, 8, 11, 14, 1, 11, 13, 2, 7, 5, 12, 9, 10, 6, 8, 8, 1, 2, 9, 14, 10, 4, 2, 11, 9, 14, 3, 10, 10, 13, 9, 7, 0, 3, 5, 13, 14, 7, 0, 5, 14, 14, 3, 5, 2, 3, 13, 7, 13, 10, 4, 12, 7, 9, 2, 5, 2, 12, 8, 11, 1, 0, 6, 10, 9, 0, 9, 10, 0, 5, 3, 9, 14, 1, 6, 13, 14, 9, 5, 12, 12, 2, 5, 6, 13, 10, 2, 3, 4, 0, 14, 6, 3, 11, 13, 2, 10, 1, 0, 1, 5, 6, 3, 1, 12, 13, 4, 2, 13, 11, 10, 6, 4, 14, 4, 11, 1, 8, 9, 13, 1, 12, 9, 9, 11, 11, 6, 13, 10, 5, 6, 8, 4, 11, 0, 6, 1, 7, 2, 6, 2, 1, 11, 5, 2, 4, 0, 1, 8, 2, 7, 10, 1, 8, 10, 10, 3, 14, 11, 12, 8, 0, 1, 5, 9, 3, 2, 6, 6, 6, 6, 7, 5, 9, 0, 9, 0, 13, 10, 7, 1, 8, 7, 9, 2, 13, 13, 1, 0, 2, 9, 10, 14, 3, 12, 9, 7, 3, 5, 7, 5, 1, 2, 0, 8, 10, 2, 5, 3, 5, 6, 12, 13, 13, 3, 12, 9, 12, 11, 5, 10, 9, 6, 10, 7, 13, 2, 2, 5, 13, 1, 5, 1, 4, 2, 2, 6, 12, 9, 0, 6, 1, 1, 11, 14, 5, 13, 14, 6, 7, 8, 5, 5, 8, 2, 3, 13, 14, 12, 1, 7, 5, 3, 8, 14, 9, 3, 1, 1, 8, 7, 13, 13, 11, 6, 1, 1, 3, 12, 11, 3, 3, 4, 10, 4, 7, 10, 2, 7, 10, 0, 8, 6, 10, 0, 9, 1, 9, 10, 1, 7, 0, 6, 7, 12, 3, 7, 12, 4, 8, 10, 1, 12, 11, 10, 1, 12, 5, 10, 7, 14, 10, 2, 10, 10, 4, 6, 5, 12, 11, 12, 5, 10, 13, 12, 11, 11, 4, 0, 10, 2, 0, 0, 7, 11, 7, 7, 8, 9, 7, 13, 12, 11, 0, 2, 0, 9, 3, 6, 5, 0, 0, 6, 14, 0, 11, 11, 0, 10, 7, 11, 1, 10, 14, 9, 6, 4, 5, 1, 12, 4, 12, 5, 14, 10, 1, 3, 2, 14, 12, 6, 9, 12, 9, 3, 11, 14, 2, 9, 3, 14, 10, 9, 4, 3, 6, 0, 3, 6, 13, 1, 2, 6, 11, 9, 5, 1, 1, 4, 0, 13, 5, 10, 2, 6, 8, 12, 8, 3, 14, 14, 13, 3, 8, 6, 14, 4, 4, 1, 9, 3, 4, 3, 12, 7, 10, 6, 10, 1, 0, 5, 12, 12, 9, 3, 14, 2, 12, 4, 0, 1, 1, 0, 10, 6, 14, 9, 14, 2, 13, 9, 5, 11, 9, 14, 8, 2, 1, 2, 0, 0, 1, 4, 8, 6, 3, 10, 5, 10, 11, 1, 0, 3, 4, 5, 0, 0, 14, 10, 3, 14, 5, 1, 5, 14, 14, 7, 0, 7, 4, 9, 7, 0, 4, 6, 5, 14, 0, 8, 5, 9, 12, 10, 11, 1, 3, 11, 6, 11, 4, 12, 7, 12, 6, 5, 0, 2, 7, 3, 2, 7, 7, 2, 3, 6, 9, 10, 14, 2, 7, 10, 7, 2, 12, 12, 5, 11, 4, 11, 3, 11, 4, 11, 14, 4, 0, 8, 13, 0, 13, 13, 1, 10, 10, 13, 8, 6, 12, 4, 7, 14, 9, 4, 1, 3, 3, 11, 5, 2, 12, 3, 3, 5, 8, 0, 7, 8, 7, 4, 5, 3, 1, 1, 2, 4, 14, 4, 6, 6, 4, 13, 2, 0, 0, 1, 12, 8, 13, 9, 3, 8, 4, 6, 14, 2, 4, 4, 8, 8, 3, 1, 3, 7, 10, 2, 2, 1, 4, 3, 4, 4, 9, 2, 9, 6, 12, 14, 14, 0, 11, 4, 3, 9, 5, 6, 12, 10, 8, 3, 2, 4, 6, 1, 6, 4, 2, 10, 7, 0, 10, 2, 7, 8, 3, 12, 13, 9, 8, 4, 2, 8, 0, 0, 12, 4, 0, 6, 13, 4, 1, 13, 13, 11, 6, 4, 4, 3, 9, 0, 14, 1, 3, 13, 11, 0, 14, 13, 2, 0, 12, 12, 7, 7, 3, 9, 12, 8, 9, 10, 5, 9, 1, 14, 13, 0, 4, 2, 2, 13, 13, 5, 4, 11, 4, 11, 11, 11, 2, 1, 1, 14, 10, 12, 10, 8, 11, 1, 12, 6, 6, 14, 10, 5, 12, 2, 4, 6, 8, 13, 5, 13, 5, 1, 7, 14, 0, 12, 11, 11, 11, 5, 9, 9, 8, 14, 11, 11, 8, 10, 0, 13, 4, 14, 8, 14, 5, 8, 13, 4, 0, 10, 7, 6, 11, 9, 14, 2, 1, 0, 4, 6, 8, 4, 10, 4, 3, 7, 2, 4, 14, 1, 3, 3, 11, 4, 14, 5, 9, 12, 2, 12, 11, 11, 7, 11, 5, 2, 12, 12, 1, 5, 11, 0, 9, 7, 5, 5, 5, 4, 7, 7, 2, 8, 2, 6, 7, 6, 7, 10, 7, 2, 9, 14, 10, 10, 12, 2, 1, 14, 11, 13, 7, 3, 9, 7, 2, 1, 3, 14, 2, 5, 7, 12, 5, 7, 9, 10, 3, 1, 12, 0, 8, 13, 12, 10, 6, 9, 7, 9, 7, 3, 11, 13, 6, 12, 1, 1, 5, 5, 4, 1, 0, 4, 7, 13, 1, 6, 3, 0, 5, 13, 13, 11, 3, 1, 3, 12, 11, 14, 3, 0, 14, 0, 11, 4, 11, 13, 1, 12, 8, 13, 12, 12, 11, 7, 0, 1, 13, 9, 11, 2, 5, 13, 9, 1, 6, 7, 11, 1, 1, 5, 0, 13, 8, 13, 8, 12, 11, 14, 11, 2, 12, 13, 1, 1, 8, 14, 14, 14, 10, 9, 11, 9, 14, 2, 6, 7, 9, 9, 10, 9, 2, 4, 14, 13, 10, 14, 10, 13, 4, 5, 3, 10, 14, 13, 4, 3, 8, 9, 14, 2, 6, 3, 4, 4, 6, 2, 0, 14, 9, 10, 2, 4, 3, 4, 11, 12, 8, 14, 8, 0, 6, 1, 8, 1, 12, 11, 3, 4, 12, 13, 14, 6, 13, 3, 3, 13, 8, 12, 5, 12, 6, 12, 8, 12, 9, 7, 0, 10, 11, 11, 12, 7, 3, 4, 1, 7, 10, 4, 3, 0, 12, 10, 5, 7, 14, 3, 14, 11, 2, 8, 13, 2, 5, 5, 1, 1, 13, 8, 7, 0, 9, 6, 6, 9, 1, 4, 5, 0, 2, 7, 9, 11, 2, 9, 0, 10, 12, 12, 6, 10, 11, 10, 9, 1, 0, 7, 1, 8, 5, 14, 6, 9, 9, 6, 13, 14, 1, 2, 7, 9, 9, 5, 0, 9, 5, 14, 10, 14, 2, 7, 2, 5, 1, 5, 4, 2, 12, 9, 0, 4, 9, 5, 9, 8, 10, 11, 4, 7, 0, 12, 3, 7, 5, 6, 7, 0, 1, 0, 9, 2, 5, 13, 5, 8, 4, 7, 4, 4, 7, 6, 13, 10, 9, 6, 0, 3, 6, 14, 5, 13, 6, 9, 7, 14, 6, 13, 13, 6, 2, 10, 6, 2, 10, 14, 4, 9, 3, 0, 12, 12, 3, 9, 1, 6, 5, 0, 10, 6, 3, 1, 12, 2, 8, 10, 5, 0, 10, 5, 7, 8, 2, 1, 8, 4, 2, 1, 13, 3, 12, 6, 13, 13, 7, 6, 11, 13, 5, 9, 9, 6, 14, 6, 8, 2, 1, 8, 10, 11, 7, 12, 5, 5, 1, 13, 9, 8, 0, 9, 5, 1, 4, 2, 8, 0, 14, 6, 7, 7, 9, 12, 6, 7, 10, 8, 10, 2, 10, 2, 11, 3, 4, 0, 0, 13, 4, 4, 0, 8, 0, 7, 4, 3, 9, 2, 11, 11, 11, 1, 4, 10, 13, 3, 9, 6, 8, 0, 0, 7, 3, 1, 5, 8, 11, 6, 6, 14, 2, 13, 9, 9, 10, 7, 0, 0, 11, 6, 12, 4, 10, 12, 1, 10, 12, 7, 9, 13, 9, 2, 4, 9, 3, 11, 3, 7, 9, 1, 0, 4, 1, 5, 12, 11, 6, 1, 0, 2, 11, 8, 1, 14, 5, 0, 8, 1, 0, 4, 10, 8, 6, 13, 2, 11, 0, 14, 14, 6, 12, 3, 8, 7, 3, 7, 3, 1, 13, 0, 5, 10, 12, 3, 10, 2, 4, 12, 4, 2, 9, 8, 1, 13, 13, 11, 9, 9, 11, 4, 12, 9, 6, 1, 14, 4, 8, 12, 11, 4, 9, 8, 1, 1, 9, 10, 6, 8, 9, 14, 6, 7, 1, 4, 11, 5, 9, 9, 11, 9, 4, 10, 4, 7, 13, 13, 6, 6, 14, 6, 13, 4, 1, 2, 7, 4, 3, 10, 8, 0, 13, 3, 0, 2, 6, 10, 13, 6, 6, 14, 12, 11, 13, 8, 12, 1, 13, 9, 14, 3, 11, 13, 3, 3, 5, 12, 11, 14, 2, 10, 14, 8, 9, 13, 7, 13, 8, 12, 13, 10, 8, 7, 12, 12, 0, 10, 0, 6, 13, 10, 12, 4, 1, 8, 14, 10, 2, 12, 8, 2, 11, 5, 10, 4, 5, 2, 10, 12, 13, 7, 3, 8, 0, 14, 0, 12, 3, 12, 4, 6, 10, 4, 8, 11, 7, 3, 6, 3, 3, 14, 6, 7, 7, 11, 13, 10, 4, 12, 2, 5, 10, 7, 8, 10, 5, 5, 5, 4, 8, 13, 14, 13, 1, 10, 1, 13, 14, 11, 1, 10, 13, 7, 10, 2, 13, 7, 8, 7, 6, 4, 6, 12, 11, 12, 2, 8, 1, 10, 8, 10, 7, 1, 7, 13, 4, 8, 4, 3, 12, 14, 7, 10, 12, 7, 9, 6, 2, 12, 14, 9, 11, 4, 12, 7, 7, 12, 13, 12, 0, 12, 2, 6, 12, 11, 11, 3, 11, 5, 11, 4, 1, 12, 1, 11, 9, 2, 9, 6, 4, 4, 4, 6, 3, 8, 2, 4, 12, 14, 9, 13, 9, 10, 1, 3, 14, 3, 7, 8, 14, 14, 3, 12, 14, 3, 0, 0, 4, 11, 0, 1, 5, 7, 2, 5, 5, 13, 3, 8, 5, 5, 9, 14, 7, 6, 4, 7, 3, 2, 8, 7, 12, 0, 13, 6, 5, 11, 4, 9, 12, 1, 2, 14, 5, 11, 9, 3, 6, 8, 12, 8, 9, 4, 14, 12, 10, 11, 10, 13, 6, 0, 10, 6, 3, 11, 11, 7, 7, 0, 10, 2, 5, 13, 4, 5, 12, 2, 1, 4, 9, 9, 9, 10, 3, 4, 10, 2, 10, 5, 10, 6, 3, 13, 12, 6, 14, 14, 8, 6, 1, 14, 9, 0, 9, 13, 9, 9, 6, 5, 4, 0, 3, 5, 3, 11, 3, 13, 6, 8, 13, 3, 12, 10, 1, 8, 7, 3, 11, 7, 10, 9, 0, 8, 7, 13, 2, 12, 9, 9, 14, 2, 4, 5, 14, 7, 12, 7, 7, 0, 3, 11, 5, 3, 13, 12, 2, 13, 6, 11, 10, 14, 11, 12, 5, 2, 4, 4, 5, 14, 1, 6, 13, 14, 4, 11, 11, 9, 0, 2, 7, 0, 1, 8, 2, 2, 13, 10, 13, 2, 8, 5, 0, 14, 13, 0, 12, 0, 0, 3, 7, 5, 0, 6, 0, 1, 12, 10, 2, 1, 10, 0, 13, 10, 11, 0, 7, 14, 7, 12, 7, 10, 14, 2, 5, 8, 1, 1, 2, 8, 2, 14, 11, 11, 8, 11, 9, 8, 13, 14, 13, 2, 1, 14, 0, 2, 13, 1, 11, 3, 2, 4, 0, 3, 4, 3, 8, 3, 1, 1, 13, 2, 8, 6, 11, 2, 6, 10, 6, 2, 8, 0, 7, 5, 14, 9, 12, 4, 5, 6, 5, 0, 10, 9, 7, 12, 6, 4, 0, 12, 10, 4, 3, 5, 14, 13, 7, 11, 3, 13, 12, 10, 9, 5, 14, 12, 6, 12, 6, 5, 11, 6, 1, 5, 1, 11, 4, 10, 8, 12, 2, 9, 5, 14, 4, 6, 5, 7, 12, 9, 11, 13, 0, 1, 2, 6, 14, 9, 6, 1, 14, 8, 13, 8, 3, 14, 12, 8, 0, 8, 12, 0, 12, 4, 0, 0, 2, 4, 14, 11, 10, 5, 7, 1, 5, 8, 2, 12, 8, 11, 13, 8, 13, 5, 9, 3, 4, 10, 13, 0, 8, 3, 13, 13, 8, 12, 7, 11, 2, 13, 7, 0, 14, 0, 13, 3, 4, 9, 7, 6, 0, 11, 6, 6, 7, 0, 0, 14, 5, 8, 3, 11, 12, 3, 12, 4, 13, 13, 13, 12, 5, 14, 3, 3, 11, 7, 5, 13, 9, 6, 11, 9, 4, 10, 5, 13, 13, 1, 10, 8, 13, 0, 1, 7, 14, 14, 14, 12, 1, 1, 10, 13, 2, 14, 5, 4, 7, 6, 2, 7, 8, 11, 13, 1, 9, 4, 14, 7, 1, 13, 14, 7, 6, 6, 7, 7, 8, 14, 2, 3, 14, 8, 5, 14, 1, 2, 6, 5, 13, 13, 13, 14, 8, 13, 2, 11, 1, 11, 7, 8, 3, 8, 9, 0, 4, 13, 5, 11, 1, 14, 1, 13, 5, 6, 6, 8, 14, 9, 3, 14, 10, 10, 11, 2, 4, 1, 0, 5, 9, 13, 6, 7, 0, 6, 14, 7, 3, 10, 14, 14, 12, 8, 0, 13, 3, 10, 8, 3, 8, 6, 11, 10, 6, 10, 4, 12, 7, 4, 10, 11, 7, 0, 10, 2, 13, 0, 4, 4, 6, 6, 0, 2, 12, 7, 14, 5, 8, 13, 8, 14, 3, 7, 13, 0, 0, 7, 8, 10, 11, 14, 2, 2, 8, 6, 11, 7, 12, 6, 2, 13, 4, 8, 7, 1, 3, 12, 9, 10, 8, 8, 8, 2, 3, 11, 5, 12, 0, 10, 6, 4, 2, 8, 14, 8, 6, 11, 8, 11, 7, 6, 7, 14, 9, 0, 1, 4, 7, 2, 3, 9, 10, 9, 10, 1, 12, 4, 3, 3, 9, 9, 8, 4, 14, 0, 13, 9, 13, 7, 14, 13, 10, 5, 9, 9, 13, 8, 2, 10, 10, 5, 6, 10, 0, 10, 9, 3, 14, 14, 10, 9, 10, 3, 1, 8, 9, 14, 12, 14, 3, 1, 11, 11, 5, 11, 2, 6, 5, 0, 4, 5, 2, 4, 10, 6, 12, 10, 3, 3, 13, 1, 12, 2, 2, 5, 14, 0, 3, 2, 9, 4, 7, 9, 8, 7, 10, 7, 10, 5, 0, 11, 8, 10, 3, 12, 2, 11, 2, 9, 4, 6, 7, 5, 5, 6, 5, 0, 13, 1, 7, 0, 0, 10, 5, 5, 12, 3, 13, 1, 12, 7, 8, 6, 11, 3, 7, 6, 4, 0, 12, 12, 1, 5, 7, 0, 0, 9, 0, 12, 14, 8, 14, 8, 5, 3, 8, 3, 7, 14, 3, 6, 10, 1, 12, 8, 14, 5, 3, 8, 14, 3, 4, 9, 4, 0, 7, 9, 0, 8, 13, 5, 7, 11, 8, 0, 3, 6, 3, 5, 6, 0, 4, 6, 5, 2, 13, 0, 8, 14, 13, 8, 14, 1, 5, 1, 1, 9, 10, 7, 2, 5, 1, 2, 13, 7, 3, 12, 2, 8, 4, 13, 6, 2, 8, 5, 4, 1, 8, 3, 3, 4, 9, 6, 10, 2, 9, 5, 12, 3, 10, 1, 2, 11, 13, 10, 12, 5, 13, 13, 0, 7, 10, 12, 4, 6, 1, 9, 4, 14, 12, 0, 11, 1, 10, 8, 3, 6, 6, 9, 11, 4, 12, 6, 11, 4, 11, 11, 2, 11, 5, 12, 8, 5, 1, 8, 14, 1, 6, 5, 14, 2, 4, 3, 6, 0, 0, 2, 10, 10, 4, 7, 8, 3, 13, 5, 11, 12, 11, 2, 13, 10, 8, 7, 8, 2, 3, 8, 12, 1, 1, 6, 8, 5, 8, 14, 10, 11, 8, 10, 4, 13, 13, 0, 10, 10, 8, 10, 12, 2, 12, 9, 4, 2, 11, 3, 9, 6, 4, 9, 14, 1, 13, 1, 8, 8, 14, 3, 13, 11, 9, 14, 4, 4, 8, 10, 6, 14, 14, 0, 2, 11, 1, 14, 2, 5, 10, 12, 11, 11, 4, 8, 10, 1, 1, 2, 12, 9, 0, 5, 3, 11, 4, 9, 12, 4, 0, 5, 6, 6, 5, 13, 8, 9, 3, 4, 5, 0, 11, 2, 6, 7, 14, 7, 11, 11, 3, 14, 8, 5, 9, 8, 13, 11, 8, 2, 11, 5, 5, 9, 12, 0, 2, 0, 9, 7, 7, 3, 10, 9, 8, 13, 3, 4, 5, 14, 11, 7, 14, 14, 8, 11, 7, 7]\n"
     ]
    }
   ],
   "source": [
    "# Use the function to obtain predictions on the test data set\n",
    "test_predictions, test_labels = get_all_predictions(model, test_loader, device)\n",
    "\n",
    "# Use the function to obtain predictions on the test data set\n",
    "print(\"Pr√©dictions:\", test_predictions)\n",
    "print(\"√âtiquettes r√©elles:\", test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ATTAQUE METHODS apply to dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Fast Sign Gradient Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the FGSM function \n",
    "def FGSM(image, epsilon, data_grad):\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    perturbed_image = image + epsilon * sign_data_grad\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading of the training data \n",
    "train_img_tensors, train_labels = prepare_set_data(train_set)\n",
    "train_img_tensors.requires_grad = True\n",
    "\n",
    "# Loading of the test data \n",
    "test_img_tensors, test_labels = prepare_set_data(test_set)\n",
    "test_img_tensors.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of the attack \n",
    "epsilon = 0.01\n",
    "\n",
    "# Run images through the model and calculate losses \n",
    "outputs_test = model(test_img_tensors.to(device))\n",
    "loss = F.cross_entropy(outputs_test, test_labels.to(device))\n",
    "\n",
    "# Reset existing gradients in the model to zero and perform backpropagation \n",
    "model.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "# Collecting gradients from input images \n",
    "data_grad = test_img_tensors.grad.data\n",
    "\n",
    "# Aply FGSM on test images \n",
    "perturbed_data_test_fgsm = FGSM(test_img_tensors, epsilon, data_grad)\n",
    "\n",
    "\n",
    "# Run training images through the model and calculate losses \n",
    "outputs_train = model(train_img_tensors.to(device))\n",
    "loss_train = F.cross_entropy(outputs_train, train_labels.to(device))\n",
    "\n",
    "# Reset the existing gradients in the model and backpropagate the training images \n",
    "model.zero_grad()\n",
    "loss_train.backward()\n",
    "\n",
    "# Collecting gradients in relation to training images \n",
    "data_grad_train = train_img_tensors.grad.data\n",
    "\n",
    "# Apply FGSM to training images \n",
    "perturbed_data_train_fgsm = FGSM(train_img_tensors, epsilon, data_grad_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy on adversarial images: 0.6466666460037231\n",
      "\n",
      "Training accuracy on adversarial images: 0.6489999890327454\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assessing the model on adversarial images\n",
    "output = model(perturbed_data_test_fgsm.to(device))\n",
    "final_loss = F.cross_entropy(output, test_labels.to(device))\n",
    "final_acc = (output.max(1)[1] == test_labels.to(device)).float().mean()\n",
    "print('\\nTest accuracy on adversarial images:', final_acc.item())\n",
    "\n",
    "# Assessing the model on the training adversarial images\n",
    "output_train = model(perturbed_data_train_fgsm.to(device))\n",
    "final_loss_train = F.cross_entropy(output_train, train_labels.to(device))\n",
    "final_acc_train = (output_train.max(1)[1] == train_labels.to(device)).float().mean()\n",
    "print('\\nTraining accuracy on adversarial images:', final_acc_train.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Projected_gradient_descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_attack(model, img, labels, eps, alpha, num_iter):\n",
    "    # Copies the image and ensures that gradients can be calculated\n",
    "    img_disturbed = img.clone().detach().requires_grad_(True)\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        # Resets gradients \n",
    "        if img_disturbed.grad is not None:\n",
    "            img_disturbed.grad.data.zero_()\n",
    "\n",
    "        # Calculates the loss \n",
    "        predictions = model(img_disturbed)\n",
    "        loss = F.cross_entropy(predictions, labels)\n",
    "\n",
    "        # Calculates gradients \n",
    "        loss.backward()\n",
    "\n",
    "        # Applies the PGD attack \n",
    "        with torch.no_grad():\n",
    "            img_disturbed.data += alpha * img_disturbed.grad.data.sign()\n",
    "            img_disturbed.data = torch.clamp(img_disturbed.data, min=img.data - eps, max=img.data + eps)\n",
    "            img_disturbed.data = torch.clamp(img_disturbed.data, min=0, max=1)\n",
    "\n",
    "    return img_disturbed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PGD to training images \n",
    "epsilon = 0.1  \n",
    "alpha = 0.01 \n",
    "num_iter = 2 \n",
    "\n",
    "# Run images through the model and calculate losses \n",
    "outputs_test = model(test_img_tensors.to(device))\n",
    "loss = F.cross_entropy(outputs_test, test_labels.to(device))\n",
    "\n",
    "# Reset existing gradients in the model to zero and perform backpropagation \n",
    "model.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "# Collecting gradients from input images \n",
    "data_grad = test_img_tensors.grad.data\n",
    "\n",
    "# Aply PGD on test images \n",
    "perturbed_data_test_pgd = pgd_attack(model, test_img_tensors, test_labels, epsilon, alpha, num_iter)\n",
    "\n",
    "\n",
    "# Run training images through the model and calculate losses \n",
    "outputs_train = model(train_img_tensors.to(device))\n",
    "loss_train = F.cross_entropy(outputs_train, train_labels.to(device))\n",
    "\n",
    "# Reset the existing gradients in the model and backpropagate the training images \n",
    "model.zero_grad()\n",
    "loss_train.backward()\n",
    "\n",
    "# Collecting gradients in relation to training images \n",
    "data_grad_train = train_img_tensors.grad.data\n",
    "\n",
    "# Apply PGD to training images \n",
    "perturbed_data_train_pgd = pgd_attack(model, train_img_tensors, train_labels, epsilon, alpha, num_iter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy on adversarial images: 0.6193333268165588\n",
      "\n",
      "Training accuracy on adversarial images: 0.6155833601951599\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assessing the model on adversarial images\n",
    "output = model(perturbed_data_test_pgd.to(device))\n",
    "final_loss = F.cross_entropy(output, test_labels.to(device))\n",
    "final_acc = (output.max(1)[1] == test_labels.to(device)).float().mean()\n",
    "print('\\nTest accuracy on adversarial images:', final_acc.item())\n",
    "\n",
    "# Assessing the model on the training adversarial images\n",
    "output_train = model(perturbed_data_train_pgd.to(device))\n",
    "final_loss_train = F.cross_entropy(output_train, train_labels.to(device))\n",
    "final_acc_train = (output_train.max(1)[1] == train_labels.to(device)).float().mean()\n",
    "print('\\nTraining accuracy on adversarial images:', final_acc_train.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Carlini&Wagner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cw_attack(model, img, label, eps, lr, max_iter):\n",
    "    # Initialisation\n",
    "    w = torch.zeros_like(img, requires_grad=True)\n",
    "    optimizer = torch.optim.Adam([w], lr=lr)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        new_img = 0.5 * (torch.tanh(w) + 1)\n",
    "        output = model(new_img)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "\n",
    "        # Calculating losses \n",
    "        real = torch.max(predicted * label)\n",
    "        other = torch.max(predicted * (1 - label))\n",
    "        loss1 = torch.clamp(other - real, min=0)\n",
    "        loss2 = torch.norm(new_img.view(new_img.size(0), -1) - img.view(img.size(0), -1), p=2)\n",
    "        loss = eps * loss1 + loss2\n",
    "\n",
    "        # Optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    adv_img = 0.5 * (torch.tanh(w) + 1)\n",
    "    return adv_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PGD to training images \n",
    "epsilon = 0.1  \n",
    "lr = 0.01\n",
    "max_iter = 2\n",
    "\n",
    "# Run images through the model and calculate losses \n",
    "outputs_test = model(test_img_tensors.to(device))\n",
    "loss = F.cross_entropy(outputs_test, test_labels.to(device))\n",
    "\n",
    "# Reset existing gradients in the model to zero and perform backpropagation \n",
    "model.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "# Collecting gradients from input images \n",
    "data_grad = test_img_tensors.grad.data\n",
    "\n",
    "# Aply PGD on test images \n",
    "perturbed_data_test_cw = cw_attack(model, test_img_tensors, test_labels, epsilon, lr, max_iter) \n",
    "\n",
    "\n",
    "# Run training images through the model and calculate losses \n",
    "outputs_train = model(train_img_tensors.to(device))\n",
    "loss_train = F.cross_entropy(outputs_train, train_labels.to(device))\n",
    "\n",
    "# Reset the existing gradients in the model and backpropagate the training images \n",
    "model.zero_grad()\n",
    "loss_train.backward()\n",
    "\n",
    "# Collecting gradients in relation to training images \n",
    "data_grad_train = train_img_tensors.grad.data\n",
    "\n",
    "# Apply PGD to training images \n",
    "perturbed_data_train_cw = cw_attack(model, train_img_tensors, train_labels, epsilon,lr, max_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy on adversarial images: 0.06666667014360428\n",
      "\n",
      "Training accuracy on adversarial images: 0.06666667014360428\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assessing the model on adversarial images\n",
    "output = model(perturbed_data_test_cw.to(device))\n",
    "final_loss = F.cross_entropy(output, test_labels.to(device))\n",
    "final_acc = (output.max(1)[1] == test_labels.to(device)).float().mean()\n",
    "print('\\nTest accuracy on adversarial images:', final_acc.item())\n",
    "\n",
    "# Assessing the model on the training adversarial images\n",
    "output_train = model(perturbed_data_train_cw.to(device))\n",
    "final_loss_train = F.cross_entropy(output_train, train_labels.to(device))\n",
    "final_acc_train = (output_train.max(1)[1] == train_labels.to(device)).float().mean()\n",
    "print('\\nTraining accuracy on adversarial images:', final_acc_train.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Visualisation of the attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displays an image before and after disruption\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Defining the function for displaying an image\n",
    "def imshow(img):\n",
    "    npimg = img.detach().numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "#Definition of the function for displaying an image before and after disturbance\n",
    "def show_images(image, perturbed_image):\n",
    "    img = image[0]\n",
    "    perturbed_img = perturbed_image[0]\n",
    "    imshow(img.cpu())\n",
    "    imshow(perturbed_img.cpu())\n",
    "\n",
    "# Display an image before and after disturbance\n",
    "show_images(test_img_tensors, perturbed_data_test_fgsm)\n",
    "show_images(test_img_tensors, perturbed_data_test_pgd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Apply defence Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- II - 1 . Implementation 1 of Adversariables Training (Goodfellow et al.(2015)) \n",
    "- II - 2 . Implementation 2 of Adversariables Training (Goodfellow et al.(2015))\n",
    "- II - 3 . Implemtation of Defensive GAN (Samangouei et al. (2018))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II - 1 . Implementation 1 of Adversariables Training (Goodfellow et al.(2015)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\n",
    "\\theta^* = arg\\min_{\\theta}  \\lambda  J(\\theta, x, y) + (1-\\lambda)  J(\\theta, x', y) \n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_loss(model, clean_data, adversary_data, labels, alpha, loss_fn, device):\n",
    "    # Calculation of the loss on clean examples\n",
    "    clean_outputs = model(clean_data)\n",
    "    clean_loss = loss_fn(clean_outputs, labels)\n",
    "\n",
    "    # Calculating the loss on adversarial examples\n",
    "    adversary_outputs = model(adversary_data)\n",
    "    adversary_loss = loss_fn(adversary_outputs, labels)\n",
    "\n",
    "    # Combination of the two losses \n",
    "    combined_loss = alpha * clean_loss + (1 - alpha) * adversary_loss\n",
    "    return combined_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the function for evaluating performance on perturbed_data_test\n",
    "def validate_perturbed(model, perturbed_data_test, test_labels, device):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = perturbed_data_test.shape[0]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Make sure that perturbed_data_test is on the correct device\n",
    "        perturbed_data_test = perturbed_data_test.to(device)\n",
    "        test_labels = test_labels.to(device)\n",
    "        \n",
    "        # Run the disturbed images through the model\n",
    "        outputs = model(perturbed_data_test)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_correct = (predicted == test_labels).sum().item()\n",
    "    \n",
    "    accuracy = total_correct / total_samples\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "    accuracy = total_correct / total_samples\n",
    "    return accuracy\n",
    "\n",
    "# List of different lambda values to test\n",
    "lambda_values = np.linspace(0, 1, num=5)  # for example: 0, 0.25, 0.5, 0.75, 1\n",
    "\n",
    "# Dictionary to save accuracy for each lambda value\n",
    "lambda_accuracies = {}\n",
    "num_epochs = 2\n",
    "for lambda_value in lambda_values:\n",
    "    # Reset the model and optimizer\n",
    "    model = CNNModel(num_classes=15)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # Train the model with the current lambda value\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_loader:\n",
    "            images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Create adversarial examples with FGSM\n",
    "            images.requires_grad = True\n",
    "            outputs = model(images)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            images_grad = images.grad.data\n",
    "            perturbed_images = FGSM(images, epsilon, images_grad)\n",
    "            \n",
    "            # Calculate mixed loss and update weights\n",
    "            model.zero_grad()\n",
    "            loss = adversarial_loss(model, images, perturbed_images, labels, lambda_value, F.cross_entropy, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} completed for lambda={lambda_value:.2f}.\")\n",
    "\n",
    "    # Evaluate the model and save accuracy\n",
    "    accuracy = validate_model(model, test_loader, device)\n",
    "    perturbed_accuracy = validate_perturbed(model, perturbed_data_test, test_labels, device)\n",
    "\n",
    "    lambda_accuracies[lambda_value] = (accuracy, perturbed_accuracy)\n",
    "    print(f\"Validation accuracy for lambda={lambda_value:.2f}: {accuracy:.4f}\")\n",
    "    print(f'Accuracy on perturbed test data: {perturbed_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning of lamda parameter for the example\n",
    "lambda_values = list(lambda_accuracies.keys())  # Simulated lambda values, ranging from 0.0 to 1.0\n",
    "accuracy_values = [accuracy[0] for accuracy in lambda_accuracies.values()]  # Simulated accuracy for each lambda value\n",
    "perturbed_accuracy_values = [accuracy[1] for accuracy in lambda_accuracies.values()]  # Simulated perturbed accuracy for each lambda value\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(lambda_values, accuracy_values, marker='o', linestyle='-', color='blue', label='Validation Accuracy')\n",
    "plt.plot(lambda_values, perturbed_accuracy_values, marker='o', linestyle='-', color='red', label='Perturbed Accuracy')\n",
    "\n",
    "# Titles and labels\n",
    "plt.title('Effect of Lambda on Validation Accuracy and Perturbed Accuracy')\n",
    "plt.xlabel('Lambda Value')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best lamda value (weighing factor) is around 0.5 like in (Goodfellow et al.(2015))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - II - 2 . Implementation 2 of Adversariables Training (Goodfellow et al.(2015))\n",
    "Idea : trains the model on both the original data and adversarial examples generated from the original data in an alternating manner.( adverse --> originale -->adverse --> ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_training(model, train_loader, epsilon, optimizer, loss_fn, device, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_loader:\n",
    "            images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Creating adversarial examples with FGSM \n",
    "            images.requires_grad = True\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            data_grad = images.grad.data\n",
    "            perturbed_images = FGSM(images, epsilon, data_grad)\n",
    "\n",
    "            # Training with original data \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Training with adversarial  data\n",
    "            optimizer.zero_grad()\n",
    "            outputs_adv = model(perturbed_images)\n",
    "            loss_adv = loss_fn(outputs_adv, labels)\n",
    "            loss_adv.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} completed.\")\n",
    "    return model\n",
    "\n",
    "# Parameters \n",
    "epsilon = 0.1  # Parameter for FGSM\n",
    "num_epochs = 5  # Number of training epochs \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Adversarial training \n",
    "model_robust_1 = adversarial_training(model, train_loader, epsilon, optimizer, loss_fn, device, num_epochs)\n",
    "\n",
    "# Save model \n",
    "torch.save(model_robust_1.state_dict(), 'results/model_robust_1.pth')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model \n",
    "model_robust_1 = CNNModel(num_classes=15)\n",
    "model_robust_1.load_state_dict(torch.load('results/model_robust_1.pth'))\n",
    "model_robust_1.eval()\n",
    "\n",
    "# Evaluate the robust model on adversarial images \n",
    "output = model_robust_1(perturbed_data_test.to(device))\n",
    "final_loss = F.cross_entropy(output, test_labels.to(device))\n",
    "final_acc = (output.max(1)[1] == test_labels.to(device)).float().mean()\n",
    "print('\\nTest accuracy on adversarial images:', final_acc.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model \n",
    "#model_robust_1 = CNNModel(num_classes=15)\n",
    "#model_robust_1.load_state_dict(torch.load('model_robust_1.pth'))\n",
    "#model_robust_1.eval()\n",
    "\n",
    "# Evaluate the robust model on adversarial images \n",
    "output = model_robust_1(perturbed_data_train.to(device))\n",
    "final_loss = F.cross_entropy(output, train_labels.to(device))\n",
    "final_acc = (output.max(1)[1] == train_labels.to(device)).float().mean()\n",
    "print('\\nTrain accuracy on adversarial images:', final_acc.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - II - 3 . Implemtation of Defensive GAN (Samangouei et al. (2018))\n",
    "\n",
    " The strucutre of the code for **STANDARD GAN**  comme from the graded assignment from the \"Deep Learning II: Master Data Science\"\n",
    " course, completed by a member of our team (Double Cursus student ENSAE & Master Data science IP-paris).\n",
    "\n",
    " Primary Contributions:\n",
    " - The core implementation, which was subject to grading, was developed by the student.\n",
    "   Although the overall code structure was inspired by the course instructor's template.\n",
    "\n",
    " - Personal enhancements include:\n",
    "   - Correction and improvement of the original coursework.\n",
    "   - Adaptation from Keras to PyTorch to maintain consistency across our project's codebase.\n",
    "      (code from th assignment was initialy in Keras)\n",
    "\n",
    " Code Enhancement:\n",
    " - This code has been enhanced to align with the Defense-GAN (WGAN) approach, approache which is built \n",
    "   using a standard GAN.\n",
    "   as introduced in the paper \"DEFENSE-GAN: PROTECTING CLASSIFIERS AGAINST ADVERSARIAL ATTACKS USING\n",
    "   GENERATIVE MODELS\" by Pouya Samangouei, Maya Kabkab, and Rama Chellappa (2018).\n",
    "\n",
    "The code 2.Defence GAN also come from us "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. STANDARD GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the Generator network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, img_shape):\n",
    "        super(Generator, self).__init__()\n",
    "        self.img_shape = img_shape\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), *self.img_shape)\n",
    "        return img\n",
    "\n",
    "# Define the Discriminator network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        flattened = img.view(img.size(0), -1)\n",
    "        validity = self.model(flattened)\n",
    "        return validity\n",
    "\n",
    "class GAN():\n",
    "    def __init__(self, dataset_name='mnist'):\n",
    "        # Load data\n",
    "        self.img_shape = (1, 64, 64)  # for MNIST\n",
    "        self.z_dim = 100\n",
    "        self.dataset_name = dataset_name\n",
    "        self.model_file = f'models/{self.dataset_name}_gan_model.pickle'\n",
    "\n",
    "        # Define networks\n",
    "        self.generator = Generator(self.z_dim, self.img_shape)\n",
    "        self.discriminator = Discriminator(self.img_shape)\n",
    "\n",
    "        # Optimizers\n",
    "        self.optimizer_G = optim.Adam(self.generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "        self.optimizer_D = optim.Adam(self.discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "        # Loss function\n",
    "        self.adversarial_loss = nn.BCELoss()\n",
    "\n",
    "    def load_gan_data(self):\n",
    "        # MNIST Dataset\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])\n",
    "        ])\n",
    "\n",
    "        dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "        return dataloader\n",
    "\n",
    "    def train(self, epochs, train_loader, sample_interval=1000):\n",
    "        for epoch in range(epochs):\n",
    "            for i, (imgs, _) in enumerate(train_loader):\n",
    "                # Adversarial ground truths\n",
    "                valid = Variable(torch.FloatTensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n",
    "                fake = Variable(torch.FloatTensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "                # Configure input\n",
    "                real_imgs = Variable(imgs.type(torch.FloatTensor))\n",
    "\n",
    "                # -----------------\n",
    "                #  Train Generator\n",
    "                # -----------------\n",
    "                self.optimizer_G.zero_grad()\n",
    "\n",
    "                # Sample noise as generator input\n",
    "                z = Variable(torch.FloatTensor(np.random.normal(0, 1, (imgs.size(0), self.z_dim))))\n",
    "\n",
    "                # Generate a batch of images\n",
    "                gen_imgs = self.generator(z)\n",
    "\n",
    "                # Loss measures generator's ability to fool the discriminator\n",
    "                g_loss = self.adversarial_loss(self.discriminator(gen_imgs), valid)\n",
    "\n",
    "                g_loss.backward()\n",
    "                self.optimizer_G.step()\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "                self.optimizer_D.zero_grad()\n",
    "\n",
    "                # Measure discriminator's ability to classify real from generated samples\n",
    "                real_loss = self.adversarial_loss(self.discriminator(real_imgs), valid)\n",
    "                fake_loss = self.adversarial_loss(self.discriminator(gen_imgs.detach()), fake)\n",
    "                d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "                d_loss.backward()\n",
    "                self.optimizer_D.step()\n",
    "\n",
    "                print(f\"[Epoch {epoch}/{epochs}] [Batch {i}/{len(train_loader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n",
    "\n",
    "                # If at save interval => save generated image samples and model checkpoints\n",
    "                if i % sample_interval == 0:\n",
    "                    # Save image samples\n",
    "                    # Save model checkpoints                    \n",
    "                    self.save_sample_images(epoch, i)\n",
    "                    torch.save(self.generator.state_dict(), f'results/generator_epoch{epoch}_batch{i}.pth')\n",
    "                    torch.save(self.discriminator.state_dict(), f'results/discriminator_epoch{epoch}_batch{i}.pth')                \n",
    "                \n",
    "    def save_sample_images(self, epoch, batch):\n",
    "        # Generate noise\n",
    "        z = Variable(torch.FloatTensor(np.random.normal(0, 1, (25, self.z_dim))))\n",
    "\n",
    "        # Generate images from noise\n",
    "        gen_imgs = self.generator(z).detach()\n",
    "\n",
    "        # Rescale images from [-1, 1] to [0, 1] range\n",
    "        gen_imgs = (gen_imgs + 1) / 2\n",
    "\n",
    "        # Save image grid\n",
    "        save_image(gen_imgs.data, f'results/epoch{epoch}_batch{batch}.png', nrow=5, normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume test_data is already defined and processed by your prepared_data function\n",
    "# from standard_GAN import GAN\n",
    "\n",
    "gan = GAN()\n",
    "gan.train(epochs=1, train_loader=train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume test_data is already defined and processed by your prepared_data function\n",
    "gan_2 = GAN()\n",
    "gan_2.train(epochs=1, train_loader=test_dataset, sample_interval=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DEFENCE GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![algorithm](utils/algo_defence_GAN.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to optimize the latent vector\n",
    "def optimize_latent_vector(generator, input_img, latent_dim, device, steps=20, lr=0.01):\n",
    "    \"\"\"\n",
    "    Optimizes the latent vector 'z' of the generator to reconstruct the input image.\n",
    "    \n",
    "    Parameters:\n",
    "    generator (torch.nn.Module): The generator model that takes a latent vector 'z'.\n",
    "    input_img (torch.Tensor): The image to be reconstructed by the generator.\n",
    "    latent_dim (int): The dimensionality of the latent space.\n",
    "    device (torch.device): The device (CPU/GPU) on which the computation will be performed.\n",
    "    steps (int): The number of optimization steps.\n",
    "    lr (float): The learning rate for the optimizer.\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: The optimized latent vector 'z'.\n",
    "    \"\"\"\n",
    "    set_seed(42) \n",
    "    # Move the input image to the specified device\n",
    "    input_img = input_img.to(device)\n",
    "    \n",
    "    # Initialize the latent vector with random noise and enable gradient computation\n",
    "    z = Variable(torch.randn(1, latent_dim).to(device), requires_grad=True)\n",
    "    \n",
    "    # Set up the optimizer for the latent vector\n",
    "    optimizer = torch.optim.Adam([z], lr=lr)\n",
    "\n",
    "    # Optimization loop\n",
    "    for step in range(steps):\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        reconstructed_img = generator(z).squeeze(1)  # Generate the reconstructed image\n",
    "        loss = F.mse_loss(reconstructed_img, input_img)  # Compute the MSE loss\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update the latent vector 'z'\n",
    "\n",
    "    return z.detach()  # Detach 'z' from the computation graph and return\n",
    "\n",
    "# Function to reconstruct the image and classify\n",
    "def defense_gan_classifier(generator, classifier, input_img, latent_dim, device):\n",
    "    \"\"\"\n",
    "    Uses DefenseGAN approach to reconstruct the image using the generator and then classifies it using the classifier.\n",
    "    \n",
    "    Parameters:\n",
    "    generator (torch.nn.Module): The generator model for image reconstruction.\n",
    "    classifier (torch.nn.Module): The classifier model for predicting the class.\n",
    "    input_img (torch.Tensor): The input image to be reconstructed and classified.\n",
    "    latent_dim (int): The dimensionality of the latent space.\n",
    "    device (torch.device): The device (CPU/GPU) on which the computation will be performed.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing the reconstructed image, predictions, and labels.\n",
    "    \"\"\"\n",
    "    set_seed(42) \n",
    "    # Optimize the latent vector to reconstruct the input image\n",
    "    optimized_z = optimize_latent_vector(generator, input_img, latent_dim, device)\n",
    "    \n",
    "    # Reconstruct the image using the optimized latent vector\n",
    "    reconstructed_img = generator(optimized_z).squeeze(1)  # Remove extra dimension\n",
    "\n",
    "    # Prepare the reconstructed image for classification\n",
    "    reconstructed_img_for_classifier = reconstructed_img.unsqueeze(0)  # Add batch dimension if needed\n",
    "\n",
    "    # Classify the reconstructed image\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        output = classifier(reconstructed_img_for_classifier.to(device))\n",
    "        predicted = output.max(1)[1]  # Get the index of the max log-probability\n",
    "\n",
    "    return reconstructed_img, predicted.item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latent_dim(generator):\n",
    "    \"\"\"find the latent dimension of the generator\"\"\"\n",
    "    for module in generator.modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # La dimension de l'espace latent est le nombre de features en entr√©e de la premi√®re couche lin√©aire\n",
    "            return module.in_features\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using it on the test data\n",
    "Generator = gan_2.generator\n",
    "latent_dim = find_latent_dim(Generator)\n",
    "input_img = test_img_tensors[1]  # Image d'entr√©e √† d√©fendre\n",
    "reconstructed_img, prediction = defense_gan_classifier(Generator, model, input_img, latent_dim, device)\n",
    "\n",
    "print(\"Pr√©diction:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_with_defensegan(generator, classifier, data, labels, latent_dim, device):\n",
    "    \"\"\"\n",
    "    Evaluates the accuracy of a classifier model using DefenseGAN for image reconstruction.\n",
    "    \n",
    "    Parameters:\n",
    "    generator (torch.nn.Module): The generator model from DefenseGAN.\n",
    "    classifier (torch.nn.Module): The classifier model to be evaluated.\n",
    "    perturbed_data (torch.Tensor): The adversarial (perturbed) images.\n",
    "    labels (torch.Tensor): The true labels of the images.\n",
    "    latent_dim (int): The dimensionality of the latent space in DefenseGAN.\n",
    "    device (torch.device): The device (CPU/GPU) on which the computation will be performed.\n",
    "    \n",
    "    Returns:\n",
    "    float: The accuracy of the classifier on the reconstructed images.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i in range(data.size(0)):\n",
    "        input_img = data[i]  # Image d'entr√©e √† d√©fendre\n",
    "        reconstructed_img, prediction = defense_gan_classifier(generator, classifier, input_img, latent_dim, device)\n",
    "\n",
    "        # evaluate the model on the reconstructed image\n",
    "        correct += (prediction == labels[i].item())\n",
    "\n",
    "    return correct / labels.size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_test_defencegan = evaluate_accuracy_with_defensegan(Generator, model,test_img_tensors, test_labels, latent_dim, device)\n",
    "accuracy_test_defencegan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
